{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple Twitter Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Aim: To write simple python scripts to collect data from Twitter using a python wrapper of Twitter API - tweepy. Then fit the InterTweet Interval (ITI) into a power distribution, to check whether or not it follows a powerlaw (for reference of Powerlaw: http://tuvalu.santafe.edu/~aaronc/powerlaws/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Collecting Twitter Data Using Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collect a certain number of Tweets by searching a hashtag or sent within a bounding box using Twitter Stream API, and store all the collected tweets into a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Introduction to the Twitter Stream API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Twitter provides two kinds of API for developers, the REST APIs and Streaming APIs. The REST APIs provides programmatic access to read and write Twitter data. While the Streaming APIs continuously deliver new responses to REST API queries over a long-lived HTTP connection. It receives updates on the latest Tweets matching a search query, stay in sync with user profile updates, and more. In this lab, since the target is to study the inter-tweet disctibution, we need to collect real-time tweets. Thus, we will use the Stream API based on a Python implementation \"tweepy\". For more information about Twitter API, please read the information on https://dev.twitter.com/overview/documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Register: Twitter Application Managment https://apps.twitter.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Twitter APIs require to authorized request to Twitter via an OAuth endpoints. To start,\n",
    "- Login to your Twitter account\n",
    "- Go to https://apps.twitter.com/app/ \n",
    "- Create a new app, enter a valid app name and URL\n",
    "- Once done, you will go to another page.\n",
    "- First switch to the permission tab and change the permissions to read write and direct messages.\n",
    "- Then switch to the Keys and Access Tokens tab and generate the consumer key and secret, as well as the access token and secret (By selecting 'Create my access token' if needed).\n",
    "\n",
    "You are now ready to proceed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Using and Understanding JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All the Twitter API search results are returned in a JSON format. JSON is a way to encode complicated information in a platform-independent way. It could be considered the lingua franca of information exchange on the Internet. It is a rather simplistic and elegant way to encode complex data structures. To encode and decode JSON, we need the package \"json\". More about JSON: https://docs.python.org/2/library/json.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start a Twitter Stream API implemented by the Python package \"tweepy\" to collect a certain number of tweets of a specific location. (pip install tweepy if needed)\n",
    "\n",
    "Using the following python code collect 100 tweets of a location of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\rchanumo\\\\Downloads\\\\lab6\\\\lab6')\n",
    "import tweepy\n",
    "import json, time\n",
    "from IPython.display import clear_output\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Go to http://dev.twitter.com and create an app.\n",
    "# The consumer key and secret will be generated for you after\n",
    "consumer_key=\"jPmqMdr53YRCzTcOO78pUXcOH\"\n",
    "consumer_secret=\"44RY6JqCfCs9jU2lpPLPLinVl4AzOCxar1VkFWBjXxiCtz1PGS\"\n",
    "\n",
    "# After the step above, you will be redirected to your app's page.\n",
    "# Create an access token under the the \"Your access token\" section\n",
    "access_token=\"3624684564-95V17JDIbJTZ0LwFjX6kkWWoQMspzuZRZryrMEq\"\n",
    "access_token_secret=\"zmdWpt8VdU6bKo53S51Mbu6NM0nQo0q9BpPefxVBrlOLb\"\n",
    "\n",
    "# List of cities: the numbers are the centers of each city, with square -/+ 0.2, using their latitudes and\n",
    "# longitudes. Feel free to add your hometown and analyze if you want!\n",
    "\n",
    "location_list = {'New York City, New York' : [-74.00 - 0.2, 40.71 - 0.2, -74.00 + 0.2, 40.71 + 0.2],\n",
    "                 'Chicago, Illinois' : [-87.63 - 0.2, 41.88 - 0.2, -87.63 + 0.2, 41.88 + 0.2],\n",
    "                 'Bloomington, Indiana' : [-86.53 - 0.2, 39.16 - 0.2, -86.53 + 0.2, 39.16 + 0.2], \n",
    "                 'Iowa City, Iowa' : [-91.53 - 0.2, 41.47 - 0.2, -91.53 + 0.2, 41.47 + 0.2], \n",
    "                 'Ann Arbor, Michigan' : [-83.75 - 0.2, 42.28 - 0.2, -83.75 + 0.2, 42.28 + 0.2],\n",
    "                 'East Lansing, Michigan' :[-84.48 - 0.2, 42.73 - 0.2, -84.48 + 0.2, 42.73 + 0.2],\n",
    "                 'Lincoln, Nebraska' : [-96.68 - 0.2, 40.81 - 0.2, -96.68 + 0.2, 40.81 + 0.2], \n",
    "                 'Columbus, Ohio' : [-82.98 - 0.2, 39.98 - 0.2, -82.98 + 0.2, 39.98 + 0.2], \n",
    "                 'State College, Pennsylvania' : [-77.86 - 0.2, 40.79 - 0.2, -77.86 + 0.2, 40.79 + 0.2], \n",
    "                 'West Lafayette, Indiana' : [-86.91 - 0.2, 40.44 - 0.2, -86.91 + 0.2, 40.44 + 0.2], \n",
    "                 'Madison, Wisconsin' : [-89.4 - 0.2, 43.07 - 0.2, -89.4 + 0.2, 43.07 + 0.2], \n",
    "                 'Champaign, Illinois' : [-88.27 - 0.2, 40.12 - 0.2, -88.27 + 0.2, 40.12 + 0.2]\n",
    "                }\n",
    "\n",
    "# The JSON listener class.\n",
    "\n",
    "class JSONListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets are the received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, fprefix = 'NY100', max_tweets = 100):\n",
    "        self.counter = 0\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('./data/' + fprefix + '.json', 'w')\n",
    "        self.max_tweets = max_tweets\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        if self.counter < self.max_tweets:\n",
    "            self.output.write(data)\n",
    "            self.counter += 1\n",
    "          \n",
    "            clear_output()\n",
    "            print( 'Tweets collected so far: {}'.format(self.counter))\n",
    "            # Print the information\n",
    "            # Comment these lines of code when you are crawling data for assignment 3!!!\n",
    "            #'''\n",
    "        #    status = json.loads(data)\n",
    "        #    print 'Tweet No. {} at [Time = {}]'.format(self.counter, status['created_at'])\n",
    "        #    print status['text']\n",
    "            #'''\n",
    "            return True\n",
    "        else:\n",
    "            print ('Totally {} tweets collected'.format(self.max_tweets))\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print ('Error: ' + str(status))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets collected so far: 100\n",
      "Totally 100 tweets collected\n",
      "Job completed. Disconnecting stream...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    listen = JSONListener(fprefix = \"NY100\", max_tweets = 100)\n",
    "    city = location_list['New York City, New York']\n",
    "    \n",
    "    stream = Stream(auth, listen)\n",
    "    stream.filter(locations=city)\n",
    "    \n",
    "    print ('Job completed. Disconnecting stream...')\n",
    "    stream.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tweets Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fit a power distribution of the InterTweet Interval based on \n",
    "\n",
    "   1. The data collected in Exercise 1 for a location of your choice (100 tweets)\n",
    "   2. A given dataset in ./data/CU.json (This corresponds to 1000 tweets from Champaign) \n",
    "\n",
    "To facillitate the powerlaw calculation, a Python package \"powerlaw\" will be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple Analytics of ITI (InterTweet Interval) based on the data collected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data preprocessing:\n",
    "\n",
    "Import JSON file, and read the information to memory. For this, use ['timestamp_ms'] which is measured in ms. first convert the time into seconds and plot the time series distribution. Using plt.hist() and plt.show() functions to plot the distribution. And the number of bins could be set as the count of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#store the time series in an array/list, named 'data'\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, json\n",
    "import numpy as np\n",
    "#Don't use inline option if you don't want to show figure in notebook.\n",
    "#%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "fp = open('./data/NY100.json','r')\n",
    "#traverse through tweets in file and find timestamps\n",
    "for line in fp:\n",
    "    #print(type(line))\n",
    "    if line not in ['\\n' , '\\r\\n']:\n",
    "        tweet_as_dictionary = json.loads(line)\n",
    "        timestamp = int(tweet_as_dictionary['timestamp_ms'])/1000\n",
    "        data.append(timestamp)\n",
    "#print(len(data))\n",
    "plt.hist(data, bins = 100)\n",
    "plt.title('Time Series Distribution')\n",
    "plt.savefig('Timestamps_NY100.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Calculate Inter-Tweet Intervals (ITI). Note, by turning the time from ms to sec, the time series turns to be a float number series, which is a continuous distribution.  Sort the data. Then calculate inter-tweet intervals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#convert list to np array\n",
    "data = np.array(data) \n",
    "#sort timestamps and find ITI by finding differnce between consecutive timestamps\n",
    "x = np.diff(np.sort(data))\n",
    "\n",
    "#plot ITI\n",
    "plt.hist(np.sort(x), bins = 100)\n",
    "plt.title('ITI')\n",
    "plt.savefig('ITI_NY100.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fit the Distribution of ITI using Power-Law. \n",
    "\n",
    "A continuous power-law distribution is one described by a probability density function $p(x)$ such that $p(x)=C f(x)$, where $C$ is a normalization constant, $X$ is the observd value, and $f(x) = x^{-\\alpha}$.\n",
    "\n",
    "Typically, the explicit form of a power-law distribution is:\n",
    "\\begin{equation} p(x) = \\frac{\\alpha-1}{x_{min}} (\\frac{x}{x_{min}})^{-\\alpha} \\end{equation}\n",
    "where $x_{min}$ is the lower bound to the power-law distribution such that it holds $\\forall x \\geq x_{min}$ with $\\alpha \\geq 1$.\n",
    "\n",
    "Sometimes, it is much easier to consider the (complementary) cumulative distribution function (CCDF) of a power-law distributed variable, which is denoted as $\\bar{P}(x)=Pr(X\\geq x)$. In the continuous case, \n",
    "\\begin{equation}\n",
    "\\bar{P}(x)=\\int_x p(x')dx' = (\\frac{x}{x_{min}})^{-\\alpha + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the cumulative distribution function (CDF) is then simply \n",
    "\\begin{equation}\n",
    "P(x) = 1 - \\bar{P}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### To estimate the parameters in the power-law, you need to implement a function to calculate the estimator of $\\alpha$, in the continuous case, as \n",
    "\\begin{equation}\n",
    "\\hat{\\alpha}=1+n(\\sum_{i=1}^n \\ln \\frac{x_i}{x_{min}})^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_i \\geq x_{min}, n = \\text{len}(x_i>=x_{min})\n",
    "\\end{equation}\n",
    "\n",
    " \n",
    "\n",
    "In this implementation, you need to assume the parameter $x_{min}$ is given or by default is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implementation of estimator of alpha for power-law\n",
    "import numpy \n",
    "import time\n",
    "import pylab\n",
    "from numpy import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def alpha(x, xmin = 0):\n",
    "    # Your implementation here. \n",
    "    # Make sure to add a small value to the denominator of the log so that the term does\n",
    "    # not blow up for small values of x_min\n",
    "    atemp = 0\n",
    "    z = x[x>=xmin]\n",
    "    for xi in z:\n",
    "        if xi != 0:\n",
    "            #print(xi)\n",
    "            atemp = atemp + log((xi + 0.5)/(xmin + 0.5))\n",
    "            #print(atemp)\n",
    "    if atemp != 0:\n",
    "        a = 1 + (len(z)/atemp)\n",
    "    else:\n",
    "        a = 1\n",
    "       \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  Estimating the lower bound on power-law behavior, $x_{min}$\n",
    "\n",
    "The power-law fitting relies on the estimation of $x_{min}$. An efficient algorithm to estimate the lower bound is proposed by Clauset et al., and the fundamental idea behind it is: we choose the value of $\\hat{x}_{min}$ that makes the probability distribution of the measured data and the best-fit power-law model as similar as possible above $x_{min}$ (i.e., $\\forall x \\geq x_{min}$).\n",
    "\n",
    "To measure the distance between two probability distributions, the commonest distance used is Kolmogorov-Smirnov or KS statistic, which is simply the maximum distance between the CDFs of the data and the fitted model:\n",
    "\\begin{equation}\n",
    "D=\\max_{x\\geq x_{min}} |S(x)-P(x)|\n",
    "\\end{equation}\n",
    "and the best $x_{min}$ is achieved by\n",
    "\\begin{equation}\n",
    "\\hat{x}_{min} = \\arg\\min_{x_{min} \\in X} D_{x_{min}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, a function to calculate the KS statistic is given, as kstest(xmin, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def kstest(xmin,x):\n",
    "    x = x[x>=xmin]\n",
    "    n = len(x)\n",
    "    if n == 0: return numpy.inf\n",
    "    # calculate the best power-law fit\n",
    "    a = alpha(x, xmin)\n",
    "    # Actual CDF\n",
    "    cx = numpy.arange(n,dtype='float')/float(n)\n",
    "    # CDF of the fitted power-law distribution\n",
    "    cf = 1-(xmin/x)**(a-1)\n",
    "    ks = max(abs(cf-cx))\n",
    "    return ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Use the above functions, to implement the power-law fitting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plfit(x):\n",
    "    # Your implementation here, ensure x is sorted\n",
    "    '''\n",
    "    Tips: choose x_min candidate from\n",
    "    xmins,argxmins = numpy.unique(x,return_index=True)\n",
    "    search the candidate xmins to determine the best xmin and alpha\n",
    "    and use the function numpy.argmin() to select minimal value\n",
    "    '''\n",
    "   \n",
    "    # The calculation of Likelihood is: L = n*log((alpha-1)/xmin) - alpha*sum(log(z/xmin)), with z = x[x>xmin]\n",
    "    xmins,argxmins = numpy.unique(x,return_index=True);\n",
    "    #initialize ks to a big value\n",
    "    ks = float('inf')\n",
    "    for xmin in xmins:\n",
    "        if xmin != max(x):\n",
    "            #for eash xmin find ks\n",
    "            ks_temp = kstest(xmin,x)\n",
    "            #print(ks_temp)\n",
    "            if(ks > ks_temp):\n",
    "                # find xmin which results in least ks\n",
    "                ks = ks_temp\n",
    "                x_min = xmin\n",
    "    \n",
    "    \n",
    "    a = alpha(x,x_min)\n",
    "    z = x[x>x_min]\n",
    "    n = len(x)\n",
    "    L = n*log((a-1)/x_min) - a*sum(log(z/x_min))\n",
    "    # Return value is (xmin, a, ks, L)\n",
    "    # a - alpha, ks - KS distance, L - likelihood\n",
    "    return (x_min, a, ks, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test the Goodness of PowerLaw fitting\n",
    "\n",
    "Given an observed data set and a hypothesized power-law distribution from which the data are drawn, we would like to know whether the hypothesis is a plausible one, given the data. A standard approach is to use a goodness-of-fit test, which generate a $p$-value that quantifies the plausibility of the hypothesis. \n",
    "\n",
    "The basic idea of such test is based on measurement of the \"distance\" between the distribution of empirical data and the hypothesized model. This distance is compared with distance measurements for comparable synthetic data sets drawn from the same model, and the p-value is defined to be the fraction of the synthetic distances that are larger than the empirical distance.\n",
    "\n",
    "If $p$ is large (close to 1), then the difference between the empirical data and the model can be attributed to the statistical fluctuations alone; if it is small, the model is not a plausible fit to the data. Typically, we use $0.1$ as threshold, that is if $p \\geq 0.1$, the model fitting is plausible, otherwise not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the following is a function to test the $p$-value, use the function to test your power-law fitting, where xmin, alpha, ks is the returned value of plfit for the empirical data. And returned value is p-value and the ks vector for the random generated synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy.random as npr\n",
    "\n",
    "def test_pl(x, xmin, alpha, ks, niter=1e2, print_timing=False):\n",
    "    \"\"\"\n",
    "    Monte-Carlo test to determine whether distribution is consistent with a power law\n",
    "\n",
    "    Runs through niter iterations of a sample size identical to the input sample size.\n",
    "\n",
    "    Will randomly select values from the data < xmin.  The number of values selected will\n",
    "    be chosen from a uniform random distribution with p(<xmin) = n(<xmin)/n.\n",
    "\n",
    "    Once the sample is created, it is fit using above methods, then the best fit is used to\n",
    "    compute a Kolmogorov-Smirnov statistic.  The KS stat distribution is compared to the \n",
    "    KS value for the fit to the actual data, and p = fraction of random ks values greater\n",
    "    than the data ks value is computed.  If p<.1, the data may be inconsistent with a \n",
    "    powerlaw.  A data set of n(>xmin)>100 is required to distinguish a PL from an exponential,\n",
    "    and n(>xmin)>~300 is required to distinguish a log-normal distribution from a PL.\n",
    "    For more details, see figure 4.1 and section\n",
    "\n",
    "    **WARNING** This can take a very long time to run!  Execution time scales as \n",
    "    niter * setsize\n",
    "\n",
    "    \"\"\"\n",
    "    niter = int(niter)\n",
    "\n",
    "    ntail = sum(x >= xmin)\n",
    "    ntot = len(x)\n",
    "    nnot = ntot-ntail              # n(<xmin)\n",
    "    pnot = nnot/float(ntot)        # p(<xmin)\n",
    "    nonpldata = x[x<xmin]\n",
    "    nrandnot = sum( npr.rand(ntot) < pnot ) # randomly choose how many to sample from <xmin\n",
    "    nrandtail = ntot - nrandnot         # and the rest will be sampled from the powerlaw\n",
    "\n",
    "    ksv = []\n",
    "    if print_timing: deltat = []\n",
    "    for i in range(niter):\n",
    "        # first, randomly sample from power law\n",
    "        # with caveat!  \n",
    "        nonplind = numpy.floor(npr.rand(nrandnot)*nnot).astype('int')\n",
    "        fakenonpl = nonpldata[nonplind]\n",
    "        randarr = npr.rand(nrandtail)\n",
    "        fakepl = randarr**(1/(1-alpha)) * xmin \n",
    "        fakedata = numpy.concatenate([fakenonpl,fakepl])\n",
    "        if print_timing: t0 = time.time()\n",
    "        # second, fit to powerlaw\n",
    "        TEST = plfit(fakedata)\n",
    "        # Get the ks of the fakedata fitting\n",
    "        ksv.append(TEST[2])\n",
    "        # Print time information if needed\n",
    "        if print_timing: \n",
    "            deltat.append( time.time() - t0 )\n",
    "            print (\"Iteration %i: %g seconds\" % (i, deltat[-1]))\n",
    "    \n",
    "    ksv = numpy.array(ksv)\n",
    "    p = (ksv>ks).sum() / float(niter)\n",
    "\n",
    "    print (\"p(%i) = %0.3f\" % (niter,p))\n",
    "    \n",
    "    if print_timing: print (\"Iteration timing: %g +/- %g\" % (numpy.mean(deltat),numpy.std(deltat)))\n",
    "\n",
    "    return p, ksv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Perform the power-law fitting. Obtain its goodness-of-fit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(100) = 0.000\n",
      "3.27799987793\n",
      "7.99236083177\n",
      "0.895691729171\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Output xmin, a, ks of best fit, and p-value.\n",
    "(xmin, a, ks, L) = plfit(x) \n",
    "(p , ksv) = test_pl(x, xmin, a, ks)\n",
    "print(xmin)\n",
    "print(a)\n",
    "print(ks)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Fill out your answers here,  for the location of your choice. \n",
    "xmin = 3.27799987793\n",
    "\n",
    "a = 7.99236083177\n",
    "\n",
    "ks_value = 0.895691729171\n",
    "\n",
    "p_value = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plotting: Plot the CDF of the input data and the fitted power-law distribution, in a log-log chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the following code to plot the CDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotcdf(x, xmin, alpha, pointcolor='k', pointmarker='+', filename=''):\n",
    "    \"\"\"\n",
    "    Plots CDF and powerlaw\n",
    "    \"\"\"\n",
    "    # Generate CDF of x\n",
    "    x=numpy.sort(x)\n",
    "    n=len(x)\n",
    "    xcdf = numpy.arange(n,0,-1,dtype='float')/float(n)\n",
    "    \n",
    "    q = x[x>=xmin]\n",
    "    fcdf = (q/xmin)**(1-alpha)\n",
    "    nc = xcdf[argmax(x>=xmin)]\n",
    "    fcdf_norm = nc*fcdf\n",
    "\n",
    "    # Draw the fitted distribution as a line\n",
    "    D_location = argmax(xcdf[x>=xmin]-fcdf_norm)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.loglog(x,xcdf,marker=pointmarker,color=pointcolor)\n",
    "    plt.loglog(q,fcdf_norm,'r')\n",
    "\n",
    "    if filename == '':\n",
    "        plt.show()\n",
    "    else:\n",
    "        # plt.show()\n",
    "        plt.savefig('3_'+filename) \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Call plotcdf() to plot the distribution here\n",
    "# Remember to save your figure into file\n",
    "# If filename = '' or no filename provided, it will be not be saved.\n",
    "\n",
    "plotcdf(x, xmin, a, 'k', '+', filename='CDF_NY100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### About the fit\n",
    "\n",
    "Do you think the fit is good? \n",
    "\n",
    "I dont think it is good fit because the p value=0.01 which is much less than 0.1\n",
    "Also from the CDF plot showing the CDF of data after applying powerfit model(red line) and CDF of actual data( in blue) we see that they dont fit each other very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comparing with other models\n",
    "\n",
    "Besides power-law, there are several other popular distributions widely used, such as exponential $f(x)=e^{-\\lambda x}$, $C=\\lambda e^{\\lambda x_{min}}$, and log-normal distribution, etc.\n",
    "\n",
    "We suggest to use an public available powerlaw fitting python package, the 'powerlaw' (which can be found in either pip or Canopy package manager). Also install mpmath if you are encountering errors.\n",
    "\n",
    "Fit the data using exponential distribution, and compare with the power-law fitting by computing the likelihood ratio test. For more information about how to use powerlaw package, please see:\n",
    "\n",
    "http://nbviewer.ipython.org/gist/anonymous/bb4e1dfafd9e90d5bc3d\n",
    "\n",
    "http://pythonhosted.org/powerlaw/#powerlaw.Fit.distribution_compare\n",
    "\n",
    "###### Hints:\n",
    "1.  Use fit.distribution_compare() function to compare fittings between power_law and lognormal distribution. The output will be which kind of distribution fits the data better\n",
    "2. Use plot_ccdf() function to draw the fitting. \n",
    "   * use fit.plot_ccdf() to plot the empiricial data distribution\n",
    "   * use fit.power_law.plot_ccdf() to plot the power_law fitting curve\n",
    "   * use fit.lognormal.plot_ccdf() to plot the log fitting curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "\n",
    "fit = powerlaw.Fit(x, discrete=False)\n",
    "# Using the functions \n",
    "# 1. fit.distribution_compare() to compare \n",
    "log_likelihood= fit.distribution_compare('power_law', 'lognormal')[0]\n",
    "print ('Log likelihood is '+ str(log_likelihood))\n",
    "\n",
    "# plot empirical CCDF, power law CCDF and log normal CCDFs\n",
    "fig = fit.plot_ccdf(color='b', linewidth=2 , label='Empirical Data')\n",
    "fit.power_law.plot_ccdf(color='r', linewidth=2, ax=fig , label='Power law') \n",
    "fit.lognormal.plot_ccdf(color='g', linewidth=2, ax=fig , label='Log normal') \n",
    "\n",
    "fig.set_ylabel(r\"$p(X\\geq x)$\")\n",
    "fig.set_xlabel(r\"ITI\")\n",
    "handles, labels = fig.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=3)\n",
    "plt.show()\n",
    "plt.savefig('4_compare_plots.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Based on the plot you get above, which fit is better?\n",
    "As x increases we see that the plot corresponding to power_law deviates more from the actual distribution as compared to log normal. therefore, The lognormal fit is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Based on the log-likelihoods you get above, which fit is better?\n",
    "Since the value of Log liklihood ratio is -2.63748431219 and therefore negative the second distrbution i.e log normal gives a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Repeat the process for the data given in ./data/CU.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data contains 1,000 tweets sent in Champaign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Get timestamps from data\n",
    "data = []\n",
    "fp = open('./data/CU.json','r')\n",
    "for line in fp:\n",
    "    #print(type(line))\n",
    "    if line not in ['\\n' , '\\r\\n']:\n",
    "        tweet_as_dictionary = json.loads(line)\n",
    "        timestamp = int(tweet_as_dictionary['timestamp_ms'])/1000\n",
    "        data.append(timestamp)\n",
    "\n",
    "#find ITI\n",
    "data = np.array(data) \n",
    "x = np.diff(np.sort(data))\n",
    "\n",
    "#Output xmin, a, ks of best fit, and p-value.\n",
    "(xmin, a, ks, L) = plfit(x) \n",
    "(p , ksv) = test_pl(x, xmin, a, ks)\n",
    "print(xmin)\n",
    "print(a)\n",
    "print(ks)\n",
    "print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Fill out your answers here, for Champaign:\n",
    "x_min =37.0099999905\n",
    "\n",
    "a = 19.3879084278\n",
    "\n",
    "ks_value = 0.368083264369\n",
    " \n",
    "p_value = 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### About the fit, for Champaign\n",
    "\n",
    "How does the fitted distribution compare to the original data?\n",
    "\n",
    "Do you think the fit is good? Explain:\n",
    "I think tha fit is good since the p-value(0.38) is greater than 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ITI Fitting Using the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run Twitter Crawler to collect 3,000 tweets choosing a specific location in the city list during the following week, and fit the ITI (Inter-Tweets Interval) distribution in the best model. (Tip: Choose your locations based on the pace of life!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(100) = 0.950\n",
      "5.48400020599\n",
      "22.8930968218\n",
      "0.264007466927\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "#Get timestamps from data\n",
    "data = []\n",
    "fp = open('./data/NY3000.json','r')\n",
    "for line in fp:\n",
    "    #print(type(line))\n",
    "    if line not in ['\\n', '\\r\\n']:\n",
    "        tweet_as_dictionary = json.loads(line)\n",
    "        timestamp = int(tweet_as_dictionary['timestamp_ms'])/1000\n",
    "        data.append(timestamp)\n",
    "\n",
    "#find ITI\n",
    "data = np.array(data) \n",
    "x = np.diff(np.sort(data))\n",
    "\n",
    "#Output xmin, a, ks of best fit, and p-value.\n",
    "(xmin, a, ks, L) = plfit(x) \n",
    "(p , ksv) = test_pl(x, xmin, a, ks)\n",
    "print(xmin)\n",
    "print(a)\n",
    "print(ks)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "x_min =5.48400020599\n",
    "\n",
    "a = 22.8930968218\n",
    "\n",
    "ks_value = 0.264007466927\n",
    " \n",
    "p_value =0.950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Questions: \n",
    "\n",
    "   * What's the main difference between the fittings between the data you collected (100 tweets), the 1,000 tweets, and the 3,000 tweets?\n",
    "   \n",
    "   The main difference is the p value. we seet that as the number of tweets collected increases the p value increases thus allowing the power law model to fit the data better.\n",
    "   \n",
    "   \n",
    "   * Is a low p-value more desirable than a high p-value? \n",
    "   \n",
    "   P values evaluate how well the sample data supports the idea that the null hypothesis is true. Thus, measures how compatible our data are with the null hypothesis. lower p value rejects the null hypothesis an higher p values support null hypothesis. since we are trying to fit the model to the emperical data, higher p value is prefered over lower p value. This is bacause lower p value rejects null hupothesis which menas there is diffrence between the data distribution of the fiited model and emperical data on the other hand higher p value(>0.1) data are likely with a true null threfore it is plausible to fit the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
